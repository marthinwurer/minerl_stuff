{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4810f489",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abade2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = [15, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a4289bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import minerl\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from utilities import flatten, unflatten, to_batch_shape, to_torch_channels\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import autoencoder\n",
    "from AdvancedAutoencoder import AdvancedAutoencoder\n",
    "from networks import WMAutoencoder, WM_VAE, VisionModel\n",
    "\n",
    "from dataset_preprocessing import MineRlSequenceDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b232d290",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a882e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MineRlSequenceDataset(\"data/npy_obtain_diamond\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "441f39f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1c18cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marthinwurer/.pyenv/versions/3.9.6/envs/minerl/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:64: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return default_collate([torch.as_tensor(b) for b in batch])\n",
      "/home/marthinwurer/.pyenv/versions/3.9.6/envs/minerl/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:64: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return default_collate([torch.as_tensor(b) for b in batch])\n",
      "/home/marthinwurer/.pyenv/versions/3.9.6/envs/minerl/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:64: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return default_collate([torch.as_tensor(b) for b in batch])\n",
      "/home/marthinwurer/.pyenv/versions/3.9.6/envs/minerl/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:64: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return default_collate([torch.as_tensor(b) for b in batch])\n",
      "/home/marthinwurer/.pyenv/versions/3.9.6/envs/minerl/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:64: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return default_collate([torch.as_tensor(b) for b in batch])\n",
      "/home/marthinwurer/.pyenv/versions/3.9.6/envs/minerl/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:64: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return default_collate([torch.as_tensor(b) for b in batch])\n",
      "/home/marthinwurer/.pyenv/versions/3.9.6/envs/minerl/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:64: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return default_collate([torch.as_tensor(b) for b in batch])\n",
      "/home/marthinwurer/.pyenv/versions/3.9.6/envs/minerl/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:64: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return default_collate([torch.as_tensor(b) for b in batch])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 1, 3, 64, 64])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features = next(iter(train_dataloader))\n",
    "train_features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91ccbc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionModel(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b152c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 3, 64, 64])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "povs = train_features[0]\n",
    "batch_size, timesteps, C, H, W = povs.shape\n",
    "input_data = povs.view(timesteps * batch_size, C, H, W)\n",
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b16cf1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outs = model(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5eb41c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = autoencoder.abs_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c30f488e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3129)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(outs[0], input_data, outs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59aac9ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionModel(\n",
       "  (encoder): VisionEncoder(\n",
       "    (encoder): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (1): ELU(alpha=1.0)\n",
       "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (3): ELU(alpha=1.0)\n",
       "      (4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (5): ELU(alpha=1.0)\n",
       "      (6): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (7): ELU(alpha=1.0)\n",
       "      (8): Flatten(start_dim=1, end_dim=-1)\n",
       "      (9): Linear(in_features=1024, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): VisionDecoder(\n",
       "    (decoder): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "      (1): Unflatten(dim=-1, unflattened_size=(1024, 1, 1))\n",
       "      (2): ConvTranspose2d(1024, 128, kernel_size=(5, 5), stride=(2, 2))\n",
       "      (3): ELU(alpha=1.0)\n",
       "      (4): ConvTranspose2d(128, 64, kernel_size=(5, 5), stride=(2, 2))\n",
       "      (5): ELU(alpha=1.0)\n",
       "      (6): ConvTranspose2d(64, 32, kernel_size=(6, 6), stride=(2, 2))\n",
       "      (7): ELU(alpha=1.0)\n",
       "      (8): ConvTranspose2d(32, 3, kernel_size=(6, 6), stride=(2, 2))\n",
       "      (9): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101c6737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "688a05b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfbb510",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(enumerate(train_dataloader, 0), total=len(dataset)/(BATCH_SIZE), unit=\"batch\") as t:\n",
    "    for i, data in t:\n",
    "        # unpack the data and send it to the gpu\n",
    "        povs, _, _, _, _, _ = data\n",
    "        batch_size, timesteps, C, H, W = povs.shape\n",
    "        images = povs.view(timesteps * batch_size, C, H, W).cuda()\n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
